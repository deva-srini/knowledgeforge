{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KnowledgeForge Workflow E2E Demo (Stage-by-Stage)\n",
    "\n",
    "This notebook runs each pipeline stage **individually** so you can inspect the\n",
    "intermediate outputs and understand what happens under the hood:\n",
    "\n",
    "```\n",
    "Parse -> Extract -> Transform -> Chunk -> Embed -> Index\n",
    "```\n",
    "\n",
    "**Prerequisites:**\n",
    "- `bgf_factsheet.pdf` in `../reference/` (or place any PDF in `../data/source/factsheets/`)\n",
    "- Virtual environment activated\n",
    "- Run from `knowledgeforge/notebooks/` or `knowledgeforge/backend/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Setup and imports\n",
    "import sys, os, shutil, time, json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "backend_dir = Path(\"../backend\").resolve()\n",
    "if str(backend_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(backend_dir))\n",
    "os.chdir(backend_dir)\n",
    "\n",
    "from app.core.config import KnowledgeForgeConfig, load_config\n",
    "from app.core.workflow_config import load_workflow\n",
    "from app.db.session import get_engine, get_session_factory, init_db, reset_globals\n",
    "from app.models.database import Document, WorkflowRun, WorkflowStage\n",
    "from app.services.parsing import DocumentParser\n",
    "from app.services.extraction import ContentExtractor, ContentType\n",
    "from app.services.transformation import ContentTransformer\n",
    "from app.services.chunking import StructureAwareChunker\n",
    "from app.services.embedding import Embedder\n",
    "from app.services.indexing import ChromaIndexer\n",
    "from app.services.filewatcher import compute_file_hash\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global config\n",
      "  chunk_size_tokens : 512\n",
      "  default_collection: default\n",
      "\n",
      "Workflow 'fund_factsheet' overlay\n",
      "  chunk_size_tokens : 256\n",
      "  chunk_overlap     : 25\n",
      "  default_collection: fund_factsheets\n",
      "  watch_folder      : ./data/source/factsheets\n",
      "  file_patterns     : ['*.pdf']\n",
      "\n",
      "Merged service config\n",
      "  chunk_size_tokens : 256\n",
      "  default_collection: fund_factsheets\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load global config + workflow overlay\n",
    "reset_globals()\n",
    "config = load_config()\n",
    "wf_config = load_workflow(\"fund_factsheet\", config)\n",
    "\n",
    "# Build the merged service config (same logic as WorkflowOrchestrator)\n",
    "svc_config = KnowledgeForgeConfig(\n",
    "    source=wf_config.source,\n",
    "    processing=wf_config.processing,\n",
    "    indexing=wf_config.indexing,\n",
    "    database=config.database,\n",
    "    observability=config.observability,\n",
    ")\n",
    "\n",
    "print(\"Global config\")\n",
    "print(f\"  chunk_size_tokens : {config.processing.chunking.chunk_size_tokens}\")\n",
    "print(f\"  default_collection: {config.indexing.default_collection}\")\n",
    "print(f\"\\nWorkflow '{wf_config.name}' overlay\")\n",
    "print(f\"  chunk_size_tokens : {wf_config.processing.chunking.chunk_size_tokens}\")\n",
    "print(f\"  chunk_overlap     : {wf_config.processing.chunking.chunk_overlap_tokens}\")\n",
    "print(f\"  default_collection: {wf_config.indexing.default_collection}\")\n",
    "print(f\"  watch_folder      : {wf_config.source.watch_folder}\")\n",
    "print(f\"  file_patterns     : {wf_config.source.file_patterns}\")\n",
    "print(f\"\\nMerged service config\")\n",
    "print(f\"  chunk_size_tokens : {svc_config.processing.chunking.chunk_size_tokens}\")\n",
    "print(f\"  default_collection: {svc_config.indexing.default_collection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File   : bgf_factsheet.pdf\n",
      "Size   : 229,656 bytes\n",
      "SHA-256: 73734621591939a6999e5336...\n",
      "Doc id : 11c41a4d-76f..., version=9\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialise database + prepare document\n",
    "engine = get_engine(config.database.url)\n",
    "init_db(engine)\n",
    "session_factory = get_session_factory(engine)\n",
    "\n",
    "# Copy reference PDF into factsheets folder\n",
    "reference_pdf = Path(\"../reference/bgf_factsheet.pdf\").resolve()\n",
    "factsheets_dir = Path(\"../data/source/factsheets\").resolve()\n",
    "factsheets_dir.mkdir(parents=True, exist_ok=True)\n",
    "target_pdf = factsheets_dir / reference_pdf.name\n",
    "shutil.copy2(str(reference_pdf), str(target_pdf))\n",
    "\n",
    "file_hash = compute_file_hash(str(target_pdf))\n",
    "print(f\"File   : {target_pdf.name}\")\n",
    "print(f\"Size   : {target_pdf.stat().st_size:,} bytes\")\n",
    "print(f\"SHA-256: {file_hash[:24]}...\")\n",
    "\n",
    "session = session_factory()\n",
    "existing = (\n",
    "    session.query(Document)\n",
    "    .filter_by(file_name=target_pdf.name, workflow_id=\"fund_factsheet\")\n",
    "    .order_by(Document.version.desc())\n",
    "    .first()\n",
    ")\n",
    "new_version = (existing.version + 1) if existing else 1\n",
    "\n",
    "doc = Document(\n",
    "    file_name=target_pdf.name,\n",
    "    file_path=str(target_pdf),\n",
    "    file_type=\"pdf\",\n",
    "    version=new_version,\n",
    "    file_hash=file_hash,\n",
    "    workflow_id=\"fund_factsheet\",\n",
    "    status=\"pending\",\n",
    ")\n",
    "session.add(doc)\n",
    "session.commit()\n",
    "session.refresh(doc)\n",
    "session.close()\n",
    "print(f\"Doc id : {doc.id[:12]}..., version={doc.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/knowledgeforge/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time          : 137.9s\n",
      "Pages         : 4\n",
      "Est. tokens   : 3,373\n",
      "Content types : {'section_header': 24, 'text': 100, 'page_footer': 5, 'page_header': 1, 'table': 4, 'picture': 10}\n",
      "Raw text len  : 14,171 chars\n",
      "\n",
      "Per-page breakdown:\n",
      "  Page 1: {'section_header': 9, 'text': 63, 'table': 2, 'picture': 2}\n",
      "  Page 2: {'section_header': 4, 'text': 9, 'page_footer': 2, 'table': 1, 'picture': 4}\n",
      "  Page 3: {'page_header': 1, 'section_header': 5, 'text': 15, 'page_footer': 2, 'table': 1, 'picture': 3}\n",
      "  Page 4: {'section_header': 6, 'text': 13, 'page_footer': 1, 'picture': 1}\n",
      "\n",
      "Raw text preview (first 500 chars):\n",
      "## BGF World Technology Fund\n",
      "\n",
      "## A2 U.S. Dollar BlackRock Global Funds\n",
      "\n",
      "December 2025\n",
      "\n",
      "Unless otherwise stated, Performance, Portfolio Breakdowns and Net Asset information as at: 31-Dec-2025.\n",
      "\n",
      "## INVESTMENT OBJECTIVE\n",
      "\n",
      "The   World   Technology   Fund   seeks   to   maximise   total   return.   The   Fund   invests globally  at   least   70%   of   its   total   assets   in   the   equity   securities   of   companies whose predominant economic activity is in the technology sector.\n",
      "\n",
      "## CUMULATIVE \n"
     ]
    }
   ],
   "source": [
    "# Stage 1: PARSE (Docling document conversion)\n",
    "parser = DocumentParser(svc_config)\n",
    "t0 = time.time()\n",
    "parse_result = parser.parse(doc.file_path)\n",
    "parse_time = time.time() - t0\n",
    "\n",
    "print(f\"Time          : {parse_time:.1f}s\")\n",
    "print(f\"Pages         : {parse_result.page_count}\")\n",
    "print(f\"Est. tokens   : {parse_result.estimated_token_count:,}\")\n",
    "print(f\"Content types : {dict(parse_result.content_types)}\")\n",
    "print(f\"Raw text len  : {len(parse_result.raw_text):,} chars\")\n",
    "\n",
    "print(\"\\nPer-page breakdown:\")\n",
    "for ps in parse_result.structure:\n",
    "    print(f\"  Page {ps.page_number}: {dict(ps.content_types)}\")\n",
    "\n",
    "print(f\"\\nRaw text preview (first 500 chars):\")\n",
    "print(parse_result.raw_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_doc = parse_result.raw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorOptions, PdfPipelineOptions, TableFormerMode, TableStructureOptions\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from IPython.display import display, Image as IPImage\n",
    "import io\n",
    "\n",
    "# Re-parse with picture image generation enabled\n",
    "pdf_opts = PdfPipelineOptions(\n",
    "    do_ocr=False,\n",
    "    force_backend_text=True,\n",
    "    do_table_structure=True,\n",
    "    table_structure_options=TableStructureOptions(mode=TableFormerMode.FAST),\n",
    "    do_picture_classification=False,\n",
    "    do_picture_description=False,\n",
    "    generate_page_images=True,   # needed for get_image() to work\n",
    "    generate_picture_images=True,\n",
    "    accelerator_options=AcceleratorOptions(device=\"auto\"),\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts)}\n",
    ")\n",
    "result = converter.convert(str(target_pdf))\n",
    "doc_with_images = result.document\n",
    "\n",
    "# Display all pictures\n",
    "for i, pic in enumerate(doc_with_images.pictures):\n",
    "    page_no = pic.prov[0].page_no if pic.prov else \"?\"\n",
    "    print(f\"\\n--- Picture {i} (page {page_no}) ---\")\n",
    "    img = pic.get_image(doc_with_images)\n",
    "    if img:\n",
    "        display(img)\n",
    "    else:\n",
    "        print(\"  (no image data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time            : 0.03s\n",
      "Items extracted : 98\n",
      "By type         : {'image_description': 10, 'text': 84, 'table': 4}\n",
      "By page         : {'p1': 67, 'p2': 13, 'p3': 4, 'p4': 14}\n",
      "\n",
      "Unique header paths (13):\n",
      "  A2 U.S. Dollar BlackRock Global Funds\n",
      "  BlackRock Global Funds\n",
      "  CALENDAR YEAR PERFORMANCE (%)\n",
      "  CALENDAR YEAR PERFORMANCE (%) > KEY FACTS\n",
      "  CUMULATIVE & ANNUALISED PERFORMANCE\n",
      "  Contact Us\n",
      "  FEES AND CHARGES *\n",
      "  IMPORTANT INFORMATION:\n",
      "  INVESTMENT OBJECTIVE\n",
      "  PORTFOLIO CHARACTERISTICS\n",
      "  PORTFOLIO MANAGERS\n",
      "  SUSTAINABILITY CHARACTERISTICS\n",
      "  SUSTAINABILITY CHARACTERISTICS DISCLOSURE:\n",
      "\n",
      "Sample extracted items (first 5):\n",
      "  [0] type=image_description  page=1  header='A2 U.S. Dollar BlackRock Global Funds'\n",
      "       text: [Image]\n",
      "  [1] type=text               page=1  header='A2 U.S. Dollar BlackRock Global Funds'\n",
      "       text: December 2025\n",
      "  [2] type=text               page=1  header='A2 U.S. Dollar BlackRock Global Funds'\n",
      "       text: Unless otherwise stated, Performance, Portfolio Breakdowns and Net Asset information as at: 31-Dec-2025.\n",
      "  [3] type=text               page=1  header='INVESTMENT OBJECTIVE'\n",
      "       text: The   World   Technology   Fund   seeks   to   maximise   total   return.   The   Fund   invests globally  at   least   ...\n",
      "  [4] type=table              page=1  header='CUMULATIVE & ANNUALISED PERFORMANCE'\n",
      "       text: |                              | CUMULATIVE (%)   | CUMULATIVE (%)   | CUMULATIVE (%)   | CUMULATIVE (%)   | CUMULATIVE ...\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: EXTRACT (structured content extraction)\n",
    "extractor = ContentExtractor(svc_config)\n",
    "t0 = time.time()\n",
    "extracted = extractor.extract(parse_result)\n",
    "extract_time = time.time() - t0\n",
    "\n",
    "type_counts = Counter(item.content_type.value for item in extracted)\n",
    "page_counts = Counter(item.page_number for item in extracted)\n",
    "\n",
    "print(f\"Time            : {extract_time:.2f}s\")\n",
    "print(f\"Items extracted : {len(extracted)}\")\n",
    "print(f\"By type         : {dict(type_counts)}\")\n",
    "print(f\"By page         : { {f'p{k}': v for k, v in sorted(page_counts.items())} }\")\n",
    "\n",
    "# Unique header paths (the document's heading hierarchy)\n",
    "headers = sorted(set(item.header_path for item in extracted if item.header_path))\n",
    "print(f\"\\nUnique header paths ({len(headers)}):\")\n",
    "for h in headers:\n",
    "    print(f\"  {h}\")\n",
    "\n",
    "# Sample items\n",
    "print(f\"\\nSample extracted items (first 5):\")\n",
    "for i, item in enumerate(extracted[:5]):\n",
    "    text_preview = item.content[:120].replace('\\n', ' ')\n",
    "    if len(item.content) > 120:\n",
    "        text_preview += '...'\n",
    "    print(f\"  [{i}] type={item.content_type.value:<18} page={item.page_number}  header='{item.header_path}'\")\n",
    "    print(f\"       text: {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractedContent(content='[Image]', content_type=<ContentType.IMAGE_DESCRIPTION: 'image_description'>, page_number=1, header_path='A2 U.S. Dollar BlackRock Global Funds', metadata={'label': 'picture'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import PIL\n",
    "\n",
    "doc = parse_result.raw_document\n",
    "for item, level in doc.iterate_items():\n",
    "    label = item.label.value if hasattr(item, \"label\") else \"\"\n",
    "    if label == \"picture\":\n",
    "        print(\"hello\")\n",
    "        img = item.get_image(doc)\n",
    "        if img:\n",
    "            display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables found: 4\n",
      "\n",
      "  Table 0 (page 1, header: 'CUMULATIVE & ANNUALISED PERFORMANCE'):\n",
      "    |                              | CUMULATIVE (%)   | CUMULATIVE (%)   | CUMULATIVE (%)   | CUMULATIVE (%)   | CUMULATIVE (%)   | ANNUALISED (%p.a.)   | ANNUALISED (%p.a.)   | ANNUALISED (%p.a.)   |\n",
      "    |------------------------------|------------------|------------------|------------------|------------------|------------------|----------------------|----------------------|----------------------|\n",
      "    |     \n",
      "    ... (1181 chars total)\n",
      "\n",
      "  Table 1 (page 1, header: 'CALENDAR YEAR PERFORMANCE (%)'):\n",
      "    |           |   2021 |   2022 |   2023 |   2024 |   2025 |\n",
      "    |-----------|--------|--------|--------|--------|--------|\n",
      "    | Fund      |   8.01 | -43.06 |  49.78 |  32.5  |  18    |\n",
      "    | Benchmark |  27.36 | -31.07 |  51.02 |  27.46 |  30.16 |\n",
      "\n",
      "Image descriptions found: 10\n",
      "  [0] page=1  header='A2 U.S. Dollar BlackRock Global Funds'  text='[Image]'\n",
      "  [1] page=1  header='CALENDAR YEAR PERFORMANCE (%)'  text='[Image]'\n",
      "  [2] page=2  header='BlackRock Global Funds'  text='[Image]'\n",
      "  [3] page=2  header='BlackRock Global Funds'  text='[Image]'\n",
      "  [4] page=2  header='BlackRock Global Funds'  text='[Image]'\n"
     ]
    }
   ],
   "source": [
    "# Inspect tables and images from extraction\n",
    "tables = [item for item in extracted if item.content_type == ContentType.TABLE]\n",
    "images = [item for item in extracted if item.content_type == ContentType.IMAGE_DESCRIPTION]\n",
    "\n",
    "print(f\"Tables found: {len(tables)}\")\n",
    "for i, tbl in enumerate(tables[:2]):\n",
    "    print(f\"\\n  Table {i} (page {tbl.page_number}, header: '{tbl.header_path}'):\")\n",
    "    preview = tbl.content[:400]\n",
    "    for line in preview.split('\\n'):\n",
    "        print(f\"    {line}\")\n",
    "    if len(tbl.content) > 400:\n",
    "        print(f\"    ... ({len(tbl.content)} chars total)\")\n",
    "\n",
    "print(f\"\\nImage descriptions found: {len(images)}\")\n",
    "for i, img in enumerate(images[:5]):\n",
    "    print(f\"  [{i}] page={img.page_number}  header='{img.header_path}'  text='{img.content[:80]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: TRANSFORM (cleaning, normalisation, markdown generation)\n",
    "transformer = ContentTransformer(svc_config)\n",
    "t0 = time.time()\n",
    "transform_result = transformer.transform(extracted, raw_document=parse_result.raw_document)\n",
    "transform_time = time.time() - t0\n",
    "\n",
    "print(f\"Time              : {transform_time:.2f}s\")\n",
    "print(f\"Items transformed : {len(transform_result.items)}\")\n",
    "print(f\"Markdown length   : {len(transform_result.document_markdown):,} chars\")\n",
    "\n",
    "# Before/After comparison\n",
    "if len(extracted) > 1:\n",
    "    print(\"\\nBefore/After comparison (item 1):\")\n",
    "    print(f\"  Original   : '{extracted[1].content[:100]}'\")\n",
    "    print(f\"  Transformed: '{transform_result.items[1].content[:100]}'\")\n",
    "\n",
    "# Show the generated markdown\n",
    "print(f\"\\nGenerated document markdown (first 1500 chars):\")\n",
    "print(\"-\" * 60)\n",
    "print(transform_result.document_markdown[:1500])\n",
    "print(\"-\" * 60)\n",
    "if len(transform_result.document_markdown) > 1500:\n",
    "    print(f\"... ({len(transform_result.document_markdown):,} chars total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4: CHUNK (structure-aware chunking)\n",
    "chunker = StructureAwareChunker(svc_config)\n",
    "t0 = time.time()\n",
    "chunks = chunker.chunk(transform_result.items)\n",
    "chunk_time = time.time() - t0\n",
    "\n",
    "total_tokens = sum(c.token_count for c in chunks)\n",
    "token_counts = [c.token_count for c in chunks]\n",
    "chunk_type_counts = Counter(c.content_type.value for c in chunks)\n",
    "\n",
    "print(f\"Time          : {chunk_time:.2f}s\")\n",
    "print(f\"Input items   : {len(transform_result.items)}\")\n",
    "print(f\"Output chunks : {len(chunks)}\")\n",
    "print(f\"Total tokens  : {total_tokens:,}\")\n",
    "print(f\"Config        : size={svc_config.processing.chunking.chunk_size_tokens}, overlap={svc_config.processing.chunking.chunk_overlap_tokens}\")\n",
    "print(f\"Token range   : min={min(token_counts)}, max={max(token_counts)}, avg={sum(token_counts)/len(token_counts):.0f}\")\n",
    "print(f\"By type       : {dict(chunk_type_counts)}\")\n",
    "\n",
    "# Full chunk listing\n",
    "print(f\"\\nAll {len(chunks)} chunks:\")\n",
    "print(f\"{'Idx':>4}  {'Type':<18}  {'Page':>4}  {'Tokens':>6}  {'Header Path':<40}  Content preview\")\n",
    "print(\"-\" * 140)\n",
    "for c in chunks:\n",
    "    text_preview = c.content[:60].replace('\\n', ' ')\n",
    "    if len(c.content) > 60:\n",
    "        text_preview += '...'\n",
    "    print(f\"{c.chunk_index:>4}  {c.content_type.value:<18}  {c.page_number:>4}  \"\n",
    "          f\"{c.token_count:>6}  {c.header_path:<40}  {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5: EMBED (sentence-transformers)\n",
    "embedder = Embedder(svc_config)\n",
    "t0 = time.time()\n",
    "embed_result = embedder.embed(chunks)\n",
    "embed_time = time.time() - t0\n",
    "\n",
    "print(f\"Time          : {embed_time:.2f}s\")\n",
    "print(f\"Model         : {svc_config.processing.embedding.model}\")\n",
    "print(f\"Batch size    : {svc_config.processing.embedding.batch_size}\")\n",
    "print(f\"Embedded      : {len(embed_result.embedded_chunks)}\")\n",
    "print(f\"Skipped       : {embed_result.skipped_count}\")\n",
    "\n",
    "if embed_result.embedded_chunks:\n",
    "    dim = len(embed_result.embedded_chunks[0].embedding)\n",
    "    print(f\"Dimension     : {dim}\")\n",
    "    print(f\"\\nSample embedding (chunk 0, first 10 values):\")\n",
    "    print(f\"  {embed_result.embedded_chunks[0].embedding[:10]}\")\n",
    "    if len(embed_result.embedded_chunks) > 1:\n",
    "        print(f\"Sample embedding (chunk 1, first 10 values):\")\n",
    "        print(f\"  {embed_result.embedded_chunks[1].embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 6: INDEX (ChromaDB upsert)\n",
    "indexer = ChromaIndexer(svc_config)\n",
    "indexer.delete_document(doc.id)  # idempotent cleanup\n",
    "\n",
    "t0 = time.time()\n",
    "index_result = indexer.index(\n",
    "    embed_result.embedded_chunks,\n",
    "    document_id=doc.id,\n",
    "    file_name=doc.file_name,\n",
    "    version=doc.version,\n",
    "    file_path=doc.file_path,\n",
    ")\n",
    "index_time = time.time() - t0\n",
    "\n",
    "print(f\"Time          : {index_time:.2f}s\")\n",
    "print(f\"Collection    : {index_result.collection_name}\")\n",
    "print(f\"Indexed       : {index_result.total_indexed} chunks\")\n",
    "print(f\"Sample IDs    : {index_result.indexed_ids[:5]}\")\n",
    "\n",
    "# Verify via ChromaDB\n",
    "import chromadb\n",
    "chroma_path = Path(svc_config.indexing.chromadb_path).resolve()\n",
    "client = chromadb.PersistentClient(path=str(chroma_path))\n",
    "coll = client.get_collection(index_result.collection_name)\n",
    "print(f\"\\nChromaDB verification: '{coll.name}' has {coll.count()} total chunks\")\n",
    "\n",
    "results = coll.peek(limit=3)\n",
    "print(f\"\\nFirst 3 stored chunks:\")\n",
    "for i, (text, meta) in enumerate(zip(results['documents'], results['metadatas'])):\n",
    "    print(f\"  [{i}] type={meta.get('content_type')}, page={meta.get('page_number')}, \"\n",
    "          f\"header='{meta.get('header_path', '')}'\")\n",
    "    print(f\"      {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing summary + pipeline overview\n",
    "total_time = parse_time + extract_time + transform_time + chunk_time + embed_time + index_time\n",
    "\n",
    "print(\"TIMING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for name, t in [(\"Parse\", parse_time), (\"Extract\", extract_time),\n",
    "                (\"Transform\", transform_time), (\"Chunk\", chunk_time),\n",
    "                (\"Embed\", embed_time), (\"Index\", index_time)]:\n",
    "    bar = '*' * max(1, int(t / total_time * 40))\n",
    "    print(f\"  {name:<12} {t:>7.2f}s  {bar}\")\n",
    "print(f\"  {'TOTAL':<12} {total_time:>7.2f}s\")\n",
    "\n",
    "print(f\"\\nPIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Document        : {doc.file_name} (v{doc.version})\")\n",
    "print(f\"  Workflow        : {wf_config.name}\")\n",
    "print(f\"  Pages           : {parse_result.page_count}\")\n",
    "print(f\"  Extracted items : {len(extracted)}\")\n",
    "print(f\"    text          : {type_counts.get('text', 0)}\")\n",
    "print(f\"    table         : {type_counts.get('table', 0)}\")\n",
    "print(f\"    image_desc    : {type_counts.get('image_description', 0)}\")\n",
    "print(f\"  Chunks          : {len(chunks)}\")\n",
    "print(f\"  Total tokens    : {total_tokens:,}\")\n",
    "print(f\"  Embedding dim   : {dim}\")\n",
    "print(f\"  Collection      : {index_result.collection_name}\")\n",
    "print(f\"  Indexed         : {index_result.total_indexed}\")\n",
    "print(f\"\\nDemo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledgeforge",
   "language": "python",
   "name": "knowledgeforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
